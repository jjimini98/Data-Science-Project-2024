{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam text message classification \n",
    "##### [스팸인지 아님 그냥 햄(메일)인지 구별하기 ]\n",
    "\n",
    "- 출처 : https://www.kaggle.com/datasets/team-ai/spam-text-message-classification\n",
    "\n",
    "스팸 메일 -> Category에 spam이라고 라벨링 됨 \n",
    "\n",
    "스팸이 아닌 경우 -> Category에 ham이라고 라벨링 됨\n",
    "\n",
    "간단할 것 같아서 가져왔는데 텍스트 분류 문제였다...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메일 내용을 토큰화할 토크나이저 선택하기 -> 일단 맨 뒤 10개만 뽑아서 결과 비교해보기\n",
    "# torch 에서 기본적으로 제공하는 토크나이저(torchtext) / spaCy / NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메일 내용을 가져와서 토큰화하기 \n",
    "torch_tokenizer = get_tokenizer('basic_english')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tt = [] ; all_wt = [] ; all_st = [] \n",
    "\n",
    "\n",
    "for contents in data['Message'][-11:-1]:\n",
    "    all_tt.append(torch_tokenizer(contents))\n",
    "    all_wt.append(word_tokenize(contents))\n",
    "    all_st.append(spacy_en.tokenizer(contents))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['get', 'me', 'out', 'of', 'this', 'dump', 'heap', '.', 'my', 'mom', 'decided', 'to', 'come', 'to', 'lowes', '.', 'boring', '.'], ['ok', 'lor', '.', '.', '.', 'sony', 'ericsson', 'salesman', '.', '.', '.', 'i', 'ask', 'shuhui', 'then', 'she', 'say', 'quite', 'gd', '2', 'use', 'so', 'i', 'considering', '.', '.', '.'], ['ard', '6', 'like', 'dat', 'lor', '.'], ['why', 'don', \"'\", 't', 'you', 'wait', \"'\", 'til', 'at', 'least', 'wednesday', 'to', 'see', 'if', 'you', 'get', 'your', '.'], ['huh', 'y', 'lei', '.', '.', '.'], ['reminder', 'from', 'o2', 'to', 'get', '2', '.', '50', 'pounds', 'free', 'call', 'credit', 'and', 'details', 'of', 'great', 'offers', 'pls', 'reply', '2', 'this', 'text', 'with', 'your', 'valid', 'name', ',', 'house', 'no', 'and', 'postcode'], ['this', 'is', 'the', '2nd', 'time', 'we', 'have', 'tried', '2', 'contact', 'u', '.', 'u', 'have', 'won', 'the', '£750', 'pound', 'prize', '.', '2', 'claim', 'is', 'easy', ',', 'call', '087187272008', 'now1', '!', 'only', '10p', 'per', 'minute', '.', 'bt-national-rate', '.'], ['will', 'ü', 'b', 'going', 'to', 'esplanade', 'fr', 'home', '?'], ['pity', ',', '*', 'was', 'in', 'mood', 'for', 'that', '.', 'so', '.', '.', '.', 'any', 'other', 'suggestions', '?'], ['the', 'guy', 'did', 'some', 'bitching', 'but', 'i', 'acted', 'like', 'i', \"'\", 'd', 'be', 'interested', 'in', 'buying', 'something', 'else', 'next', 'week', 'and', 'he', 'gave', 'it', 'to', 'us', 'for', 'free']]\n"
     ]
    }
   ],
   "source": [
    "print(all_tt) \n",
    "# 구두점, 특수문자는 다 빼버린다는게 단점인데 그래도 단어를 비교적 잘 나눈다고 볼 수 있다. \n",
    "# 대문자를 모두 다 소문자로 바꿔줘서 대문자/소문자로 인해 다른 단어로 처리되는 일은 없다는 것이 장점  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Get', 'me', 'out', 'of', 'this', 'dump', 'heap', '.', 'My', 'mom', 'decided', 'to', 'come', 'to', 'lowes', '.', 'BORING', '.'], ['Ok', 'lor', '...', 'Sony', 'ericsson', 'salesman', '...', 'I', 'ask', 'shuhui', 'then', 'she', 'say', 'quite', 'gd', '2', 'use', 'so', 'i', 'considering', '...'], ['Ard', '6', 'like', 'dat', 'lor', '.'], ['Why', 'do', \"n't\", 'you', 'wait', \"'til\", 'at', 'least', 'wednesday', 'to', 'see', 'if', 'you', 'get', 'your', '.'], ['Huh', 'y', 'lei', '...'], ['REMINDER', 'FROM', 'O2', ':', 'To', 'get', '2.50', 'pounds', 'free', 'call', 'credit', 'and', 'details', 'of', 'great', 'offers', 'pls', 'reply', '2', 'this', 'text', 'with', 'your', 'valid', 'name', ',', 'house', 'no', 'and', 'postcode'], ['This', 'is', 'the', '2nd', 'time', 'we', 'have', 'tried', '2', 'contact', 'u.', 'U', 'have', 'won', 'the', '£750', 'Pound', 'prize', '.', '2', 'claim', 'is', 'easy', ',', 'call', '087187272008', 'NOW1', '!', 'Only', '10p', 'per', 'minute', '.', 'BT-national-rate', '.'], ['Will', 'ü', 'b', 'going', 'to', 'esplanade', 'fr', 'home', '?'], ['Pity', ',', '*', 'was', 'in', 'mood', 'for', 'that', '.', 'So', '...', 'any', 'other', 'suggestions', '?'], ['The', 'guy', 'did', 'some', 'bitching', 'but', 'I', 'acted', 'like', 'i', \"'d\", 'be', 'interested', 'in', 'buying', 'something', 'else', 'next', 'week', 'and', 'he', 'gave', 'it', 'to', 'us', 'for', 'free']]\n"
     ]
    }
   ],
   "source": [
    "print(all_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Get me out of this dump heap. My mom decided to come to lowes. BORING., Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering..., Ard 6 like dat lor., Why don't you wait 'til at least wednesday to see if you get your ., Huh y lei..., REMINDER FROM O2: To get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode, This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate., Will ü b going to esplanade fr home?, Pity, * was in mood for that. So...any other suggestions?, The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free]\n"
     ]
    }
   ],
   "source": [
    "print(all_st) # 토크나이징이 된건지 모르겠네 ; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[의견]\n",
    "\n",
    "스팸인지 그냥 햄(메일)인지 구별하기 위해서 구두점이 중요한 역할을 하는지 생각해볼 필요가 있다.\n",
    "\n",
    "대화체처럼 구두점이 중요한 역할을 한다면, 구두점 자체를 살려서 가져갈 필요가 있지만 우린 스팸인지 햄인지 구별하는거라 구두점이 의미가 있는지가 궁금함\n",
    "\n",
    "우리가 평소에 받는 스팸 메일의 경우의 특징을 생각해보자.\n",
    "\n",
    "내 스팸 메일함을 살펴보면 -> 최저가 , 부업, 법률상담, 할인, 이벤트, 창업, 소득 등 특정 단어가 들어감. \n",
    "\n",
    "내가 사용하는 데이터에도 이렇게 스팸 메일이라고 분류할만한 단어가 있는지, 확인해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam 메일 개수 (전체 데이터 : 5572개) :  747\n",
      "전체 메일 중 스팸 메일의 비율 :  13.406317300789663\n"
     ]
    }
   ],
   "source": [
    "spam_contents = data[data['Category'] == \"spam\"]['Message']\n",
    "\n",
    "print(\"spam 메일 개수 (전체 데이터 : 5572개) : \", len(spam_contents))\n",
    "print(\"전체 메일 중 스팸 메일의 비율 : \", (len(spam_contents) / len(data)) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\n",
      "Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\n",
      "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow\n",
      "SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wings play Ice Hockey. Correct or Incorrect? End? Reply END SPTV\n",
      "Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\n",
      "As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589\n",
      "Urgent UR awarded a complimentary trip to EuroDisinc Trav, Aco&Entry41 Or £1000. To claim txt DIS to 87121 18+6*£1.50(moreFrmMob. ShrAcomOrSglSuplt)10, LS1 3AJ\n",
      "Did you hear about the new \"Divorce Barbie\"? It comes with all of Ken's stuff!\n",
      "Please call our customer service representative on 0800 169 6031 between 10am-9pm as you have WON a guaranteed £1000 cash or £5000 prize!\n",
      "Your free ringtone is waiting to be collected. Simply text the password \"MIX\" to 85069 to verify. Get Usher and Britney. FML, PO Box 5249, MK17 92H. 450Ppw 16\n",
      "GENT! We are trying to contact you. Last weekends draw shows that you won a £1000 prize GUARANTEED. Call 09064012160. Claim Code K52. Valid 12hrs only. 150ppm\n",
      "You are a winner U have been specially selected 2 receive £1000 or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810910p/min (18+)\n",
      "PRIVATE! Your 2004 Account Statement for 07742676969 shows 786 unredeemed Bonus Points. To claim call 08719180248 Identifier Code: 45239 Expires\n"
     ]
    }
   ],
   "source": [
    "for s in spam_contents[:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam 메일에서 token 의 개수 : 18821\n",
      "spam 메일에서 apostrophe 개수 :  121\n",
      "spam 메일의 전체 토큰 중 apostrophe의 비율 : 0.6428988895382818\n",
      "[\"c's\", \"08452810075over18's\", \"it's\", \"week's\", \"i'd\", \"ken's\", \"uk's\", \"you'll\", \"haven't\", \"i'm\", \"c's\", \"'help'\", \"c's\", \"uk's\", \"today's\", \"t's\", \"c's\", \"i'm\", \"i'm\", \"we'll\", \"c's\", \"moon's\", \"today's\", \"1000's\", \"u've\", \"don't\", \"don't\", \"don't\", \"let's\", \"1000's\", \"c's\", \"08452810075over18's\", \"c's\", \"'uptown\", \"girl'\", \"80's\", \"george's\", \"u've\", \"month's\", \"we'll\", \"t's\", \"c's\", \"uk's\", \"'help'\", \"b'tooth\", \"u've\", \"c's\", \"there's\", \"george's\", \"i'm\", \"'\", \"don't\", \"don't\", \"let's\", \"i'm\", \"t's\", \"c's\", \"uk's\", \"uk's\", \"don't\", \"don't\", \"let's\", \"t's\", \"c's\", \"\\x93it's\", \"you've\", \"c's\", \"you're\", \"you've\", \"'help'\", \"c's\", \"you've\", \"uk's\", \"rct'\", \"let's\", \"i'm\", \"won't\", \"t's\", \"c's\", \"they're\", \"100's\", \"valentine's\", \"won't\", \"it's\", \"it's\", \"you've\", \"you're\", \"moon's\", \"don't\", \"weekend's\", \"i've\", \"i'm\", \"can't\", \"'help'\", \"week's\", \"u've\", \"uk's\", \"today's\", \"i'm\", \"t's\", \"c's\", \"there's\", \"there's\", \"there's\", \"don't\", \"don't\", \"hubby's\", \"t's\", \"c's\", \"unicef's\", \"u've\", \"i'd\", \"won't\", \"it's\", \"it's\", \"moon's\", \"c's\", \"don't\", \"weekend's\", \"t's\", \"c's\"]\n"
     ]
    }
   ],
   "source": [
    "# 구두점 특히 '(어퍼스트로피) 가 스팸메일에 얼마나 들어가고 주로 어떻게 사용되는지 확인해보기 (구두점이 실제 스팸 메일을 구별하는데 필요한지 확인하기 위함)\n",
    "# 구두점을 살리는 토크나이저로 가져와보기  -> tensorflow 의 keras 사용 \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "apost = []\n",
    "all_word_counts = 0 \n",
    "\n",
    "for contents in spam_contents:\n",
    "    tokens = text_to_word_sequence(contents)\n",
    "    all_word_counts+=len(tokens)\n",
    "    for t in tokens : \n",
    "        if \"'\" in t: \n",
    "            apost.append(t)\n",
    "\n",
    "print(\"spam 메일에서 token 의 개수 :\", all_word_counts)\n",
    "print(\"spam 메일에서 apostrophe 개수 : \", len(apost))\n",
    "print(\"spam 메일의 전체 토큰 중 apostrophe의 비율 :\", (len(apost) / all_word_counts) * 100)\n",
    "print(apost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[의견]\n",
    "\n",
    "spam 메일에서 apostrophe를 가지고 있는 토큰의 비율은 0.64%로 적은 편이며 단어를 살펴봤을 때 중요한 단어는 없는 것으로 판단. (URGENT, PRIVATE, free ..)\n",
    "\n",
    "spam 메일에서 apostrophe는 삭제해도 무방할 것 같지만..? 실제 햄메일에서는 apostrophe가 중요한 역할을 할 수도 있으므로 확인 필요 \n",
    "\n",
    "햄메일에서도 그닥 중요한 역할을 하지 않는다면 apostrophe는 굳이 포함하지 않는 torchtext 를 사용해도 될 것 같다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n",
      "Even my brother is not like to speak with me. They treat me like aids patent.\n",
      "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
      "I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "Oh k...i'm watching here:)\n",
      "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "Fine if thats the way u feel. Thats the way its gota b\n",
      "Is that seriously how you spell his name?\n",
      "I‘m going to try for 2 months ha ha only joking\n",
      "So ü pay first lar... Then when is da stock comin...\n",
      "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?\n",
      "Ffffffffff. Alright no way I can meet up with you sooner?\n",
      "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\n",
      "Lol your always so convincing.\n",
      "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\n"
     ]
    }
   ],
   "source": [
    "normal_contents = data[data['Category'] != \"spam\"]['Message']\n",
    "\n",
    "for n in normal_contents[:20]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal 메일에서 token 의 개수 : 69703\n",
      "normal 메일에서 apostrophe 개수 :  1697\n",
      "normal 메일의 전체 토큰 중 apostrophe의 비율 : 2.434615439794557\n",
      "[\"don't\", \"'melle\", \"'\", \"i'm\", \"don't\", \"i've\", \"i've\", \"i'm\", \"i'm\", \"i'm\", \"mom's\", \"i'm\", \"we're\", \"i'll\", \"there's\", \"that's\", \"that's\", \"doesn't\", \"won't\", \"i'll\", \"roommate's\", \"how's\", \"you'd\", \"i'm\", \"didn't\", \"can't\", \"don't\", \"didn't\", \"i'm\", \"he's\", \"i'm\", \"didn't\", \"don't\", \"i'm\", \"i'm\", \"i'll\", \"can't\", \"ta's\", \"i'll\", \"don't\", \"i'm\", \"i'm\", \"i'm\", \"haven't\", \"don't\", \"i'm\", \"'melle\", \"'\", \"you're\", \"didn't\", \"i'll\", \"isn't\", \"he's\", \"i'm\", \"i'm\", \"i'm\", \"''\", \"b'day\", \"''\", \"he's\", \"ü'll\", \"hasn't\", \"shouldn't\", \"i'm\", \"it's\", \"you're\", \"i'm\", \"'melle\", \"'\", \"i'm\", \"see's\", \"i'm\", \"i'm\", \"one's\", \"i'm\", \"you've\", \"won't\", \"we'd\", \"where's\", \"i'm\", \"i've\", \"you've\", \"''\", \"b'day\", \"''\", \"what's\", \"haven't\", \"there's\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"it's\", \"u've\", \"i'm\", \"don't\", \"didn't\", \"i'm\", \"you're\", \"don't\", \"i'm\", \"i'm\", \"we're\", \"i'll\", \"guy's\", \"i'll\", \"it's\", \"can't\", \"it's\", \"won't\", \"aren't\", \"he'll\", \"friend's\", \"he's\", \"i'm\", \"we'll\", \"i'd\", \"that's\", \"you're\", \"i'm\", \"employer's\", \"i'll\", \"that's\", \"can't\", \"i've\", \"you're\", \"i'm\", \"i'm\", \"i'm\", \"i'm\", \"you're\", \"audrey's\", \"you're\", \"i'm\", \"did't\", \"there's\", \"that's\", \"there's\", \"i'm\", \"shouldn't\", \"we'll\", \"you've\", \"i've\", \"that's\", \"can't\", \"i'll\", \"i'll\", \"i'm\", \"she'll\", \"i'm\", \"i've\", \"i'll\", \"i'm\", \"where's\", \"that's\", \"i'll\", \"don't\", \"i'll\", \"don't\", \"i'm\", \"you're\", \"when're\", \"i've\", \"account's\", \"i'll\", \"it's\", \"dat's\", \"how's\", \"i've\", \"what's\", \"g's\", \"i'm\", \"i'm\", \"that's\", \"that's\", \"that's\", \"won't\", \"x'mas\", \"you're\", \"i'll\", \"i've\", \"doesn't\", \"'anything'\", \"'married'\", \"doesn't\", \"bloke's\", \"you're\", \"i've\", \"it's\", \"you'll\", \"i'm\", \"it's\", \"can't\", \"i'm\", \"i'll\", \"how's\", \"i'll\", \"i'm\", \"i'm\", \"mum's\", \"it's\", \"i'm\", \"it's\", \"it's\", \"you're\", \"it's\", \"that's\", \"i'm\", \"i'll\", \"i'm\", \"can't\", \"isn't\", \"i'm\", \"dealer's\", \"'\", \"mum's\", \"carlos'll\", \"won't\", \"i'll\", \"i've\", \"u're\", \"i'm\", \"i'm\", \"can't\", \"i'm\", \"'hex'\", \"it's\", \"alex's\", \"i'll\", \"sms'd\", \"don't\", \"i'm\", \"s'fine\", \"i'm\", \"didn't\", \"won't\", \"u're\", \"hadn't\", \"we've\", \"don't\", \"phone's\", \"i'm\", \"i'll\", \"they're\", \"can't\", \"don't\", \"you're\", \"couldn't\", \"she's\", \"i've\", \"isn't\", \"it's\", \"it's\", \"don't\", \"i'll\", \"ain't\", \"don't\", \"it's\", \"there's\", \"it's\", \"where's\", \"doesn't\", \"he'll\", \"how's\", \"we've\", \"that's\", \"month's\", \"i'm\", \"prashanthettan's\", \"jay's\", \"don't\", \"don't\", \"can't\", \"there's\", \"i've\", \"i've\", \"he's\", \"i'll\", \"don't\", \"i'll\", \"i'm\", \"'til\", \"she's\", \"it's\", \"u're\", \"i'm\", \"doesn't\", \"shouldn't\", \"i'm\", \"don't\", \"i've\", \"we'll\", \"ain't\", \"we're\", \"what's\", \"zaher's\", \"can't\", \"i'm\", \"didn't\", \"i'm\", \"'t\", \"ashley's\", \"did't\", \"i'm\", \"i'm\", \"i'm\", \"won't\", \"term's\", \"you'll\", \"we'll\", \"i've\", \"you're\", \"dobby's\", \"u'll\", \"isn't\", \"it's\", \"i've\", \"can't\", \"he'll\", \"i'm\", \"can't\", \"can't\", \"don't\", \"it's\", \"i'm\", \"don't\", \"can't\", \"they're\", \"hasn't\", \"it's\", \"i'll\", \"we'll\", \"you're\", \"i'm\", \"i'll\", \"mom's\", \"'its\", \"i'm\", \"it's\", \"jay's\", \"i'm\", \"they're\", \"how're\", \"we're\", \"i'm\", \"that's\", \"i'll\", \"i'll\", \"don't\", \"fr'ndship\", \"don't\", \"cali's\", \"don't\", \"i'll\", \"u'll\", \"we're\", \"i've\", \"you'll\", \"i'm\", \"i'm\", \"i'll\", \"''\", \"b'day\", \"''\", \"it's\", \"shade's\", \"she's\", \"i'm\", \"i'll\", \"i'll\", \"didn't\", \"he's\", \"there's\", \"there's\", \"i'm\", \"i'm\", \"it's\", \"i'm\", \"you'll\", \"i'm\", \"isn't\", \"there's\", \"he's\", \"i'll\", \"we're\", \"it's\", \"i've\", \"ain't\", \"i'll\", \"won't\", \"i'll\", \"i'm\", \"ü'll\", \"u'll\", \"1's\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"don't\", \"i'm\", \"i'm\", \"i'm\", \"parents'\", \"we've\", \"i've\", \"doesn't\", \"i'm\", \"what's\", \"i'll\", \"don't\", \"i'm\", \"she's\", \"i'll\", \"4'o\", \"jay's\", \"i'll\", \"it's\", \"patty's\", \"what's\", \"i'll\", \"he's\", \"he's\", \"leona's\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"don't\", \"that's\", \"i'm\", \"we're\", \"it's\", \"that's\", \"you'd\", \"doesn't\", \"i'll\", \"i've\", \"she's\", \"i'll\", \"i'll\", \"don't\", \"don't\", \"i'm\", \"haven't\", \"how's\", \"that's\", \"i'll\", \"i'm\", \"blake's\", \"'t\", \"i'm\", \"i'm\", \"it's\", \"i'm\", \"i'm\", \"i'll\", \"don't\", \"can't\", \"i'm\", \"i'm\", \"someone's\", \"i'm\", \"don't\", \"won't\", \"you'll\", \"didn't\", \"i've\", \"don't\", \"i'll\", \"how's\", \"you're\", \"i'm\", \"don't\", \"you're\", \"don't\", \"that's\", \"i'm\", \"i'm\", \"it's\", \"that's\", \"derek's\", \"it's\", \"i'll\", \"i'll\", \"let's\", \"that's\", \"i'll\", \"i'm\", \"i'll\", \"i'll\", \"we'll\", \"i'll\", \"how's\", \"'simple'\", \"don't\", \"don't\", \"i'm\", \"i'd\", \"wasn't\", \"didn't\", \"we're\", \"we're\", \"they're\", \"shouldn't\", \"don't\", \"don't\", \"'ll\", \"i'm\", \"there's\", \"i'm\", \"where's\", \"aren't\", \"it's\", \"that's\", \"it's\", \"don't\", \"i'm\", \"i'm\", \"can't\", \"don't\", \"how's\", \"doesn't\", \"haven't\", \"don't\", \"i'll\", \"'maangalyam\", \"'\", \"we're\", \"finn's\", \"i'm\", \"won't\", \"joy's\", \"joy's\", \"it's\", \"it's\", \"i'm\", \"i'm\", \"i'll\", \"i'm\", \"blake's\", \"he's\", \"i'm\", \"i'll\", \"how's\", \"dat's\", \"i'll\", \"something's\", \"couldn't\", \"you're\", \"what's\", \"you'ld\", \"that's\", \"how's\", \"i'm\", \"won't\", \"did't\", \"what's\", \"we'll\", \"didn't\", \"you'd\", \"wasn't\", \"don't\", \"wasn't\", \"don't\", \"don't\", \"can't\", \"i'm\", \"there's\", \"that's\", \"i'll\", \"knee's\", \"don't\", \"i'll\", \"i'm\", \"don't\", \"can't\", \"can't\", \"station's\", \"don't\", \"i'll\", \"there're\", \"i'm\", \"i've\", \"can't\", \"i'm\", \"i'll\", \"i'll\", \"it's\", \"don't\", \"don't\", \"idea's\", \"you're\", \"i'm\", \"i'll\", \"i've\", \"i'll\", \"you're\", \"i'm\", \"what's\", \"don't\", \"it's\", \"don't\", \"we'll\", \"i'm\", \"i'm\", \"i'll\", \"i'll\", \"i'm\", \"i'm\", \"i'll\", \"basket's\", \"i'm\", \"didn't\", \"where's\", \"i'll\", \"can't\", \"can't\", \"don't\", \"that's\", \"you'd\", \"i've\", \"don't\", \"it's\", \"i'm\", \"you're\", \"don't\", \"anybody's\", \"haven't\", \"can't\", \"i'm\", \"i'll\", \"i'm\", \"didn't\", \"i'm\", \"i'm\", \"who's\", \"isn't\", \"i'm\", \"wat's\", \"i'm\", \"i'm\", \"can't\", \"you've\", \"can't\", \"can't\", \"can't\", \"can't\", \"i'm\", \"didn't\", \"i'll\", \"i'll\", \"it's\", \"i'm\", \"i'm\", \"you're\", \"you've\", \"don't\", \"i'll\", \"wasn't\", \"it's\", \"i'm\", \"i'm\", \"he's\", \"don't\", \"table's\", \"i'm\", \"b'day\", \"did'nt\", \"''\", \"b'day\", \"''\", \"''\", \"''\", \"''ok''\", \"'\", \"everybody's\", \"i'm\", \"wat's\", \"i'm\", \"don't\", \"i'm\", \"i'm\", \"textin'\", \"i'm\", \"i'm\", \"weren't\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"god's\", \"there's\", \"no's\", \"what's\", \"i'm\", \"bb's\", \"doesn't\", \"i'll\", \"i'm\", \"it's\", \"i'm\", \"she's\", \"didn't\", \"'doctors'\", \"don't\", \"b'day\", \"did'nt\", \"i've\", \"all's\", \"roommate's\", \"i'll\", \"he's\", \"don't\", \"that's\", \"that's\", \"that's\", \"that'll\", \"don't\", \"that's\", \"that's\", \"couldn't\", \"i'll\", \"i'll\", \"i'm\", \"you've\", \"'hw\", \"'\", \"it's\", \"i'm\", \"that's\", \"month's\", \"i'll\", \"where's\", \"how's\", \"i'm\", \"i'm\", \"don't\", \"don't\", \"i'm\", \"you'd\", \"i'm\", \"i'm\", \"who's\", \"that's\", \"i'm\", \"haven't\", \"i'll\", \"anjola's\", \"that's\", \"'taxless'\", \"i'm\", \"hasn't\", \"he's\", \"doesn't\", \"can't\", \"i'll\", \"i've\", \"i'll\", \"i'm\", \"don't\", \"can't\", \"i'm\", \"i'm\", \"i'm\", \"i'd\", \"you've\", \"you'll\", \"that's\", \"don't\", \"i'm\", \"couldn't\", \"it's\", \"'if\", \"invited'\", \"i'll\", \"i've\", \"i'm\", \"it's\", \"can't\", \"i'm\", \"i'm\", \"don't\", \"i'm\", \"i'll\", \"joy's\", \"joy's\", \"i've\", \"i'll\", \"i'll\", \"he'll\", \"there's\", \"what's\", \"we'll\", \"that's\", \"how's\", \"i've\", \"i'm\", \"i'm\", \"i'll\", \"i'll\", \"it's\", \"i'll\", \"i'm\", \"'\", \"i'm\", \"didn't\", \"i'm\", \"don't\", \"you're\", \"didn't\", \"it's\", \"he's\", \"he's\", \"i'm\", \"i'll\", \"didn't\", \"you're\", \"it's\", \"can't\", \"wasn't\", \"i'm\", \"i'm\", \"dramastorm's\", \"i'm\", \"i'm\", \"that's\", \"don't\", \"we'd\", \"i'll\", \"don't\", \"taylor's\", \"don't\", \"when's\", \"can't\", \"i'm\", \"that's\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"who's\", \"i'm\", \"i'm\", \"what's\", \"i've\", \"i'm\", \"u'll\", \"didn't\", \"haven't\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"it's\", \"we'll\", \"today's\", \"you're\", \"i'm\", \"today's\", \"i'll\", \"there's\", \"i'll\", \"i've\", \"i've\", \"it's\", \"you're\", \"you're\", \"what's\", \"don't\", \"who's\", \"i'll\", \"1's\", \"i'm\", \"how's\", \"b'day\", \"did'nt\", \"''\", \"b'day\", \"''\", \"''\", \"''\", \"''ok''\", \"'\", \"she's\", \"i'm\", \"i'm\", \"fren's\", \"i'm\", \"haven't\", \"i'm\", \"i've\", \"you're\", \"i'm\", \"i'm\", \"harish's\", \"i'll\", \"didn't\", \"won't\", \"ron's\", \"it's\", \"there's\", \"i'll\", \"it's\", \"i'll\", \"what's\", \"i'll\", \"didn't\", \"i'm\", \"he's\", \"i've\", \"god's\", \"wat's\", \"hi'\", \"'xam\", \"don't\", \"i'm\", \"tm'ing\", \"i've\", \"didn't\", \"doesn't\", \"don't\", \"don't\", \"i'll\", \"there's\", \"i'm\", \"i'm\", \"y'day\", \"i'll\", \"don't\", \"i'm\", \"i'd\", \"i'm\", \"can't\", \"isn't\", \"it's\", \"did't\", \"i'm\", \"we'll\", \"we're\", \"i'll\", \"riley's\", \"how've\", \"don't\", \"l'm\", \"your's\", \"didn't\", \"don't\", \"don't\", \"don't4get2text\", \"what's\", \"don't\", \"i've\", \"i'll\", \"i'm\", \"how's\", \"don't\", \"didn't\", \"it's\", \"i'm\", \"did't\", \"haven't\", \"don't\", \"how's\", \"''\", \"pa'\", \"that's\", \"it's\", \"haven't\", \"we're\", \"i've\", \"i'm\", \"you're\", \"you're\", \"i'll\", \"he's\", \"isn't\", \"can't\", \"i'm\", \"i'm\", \"'t\", \"haven't\", \"i'll\", \"'\", \"it's\", \"didn't\", \"wouldn't\", \"she's\", \"i've\", \"i've\", \"didn't\", \"''\", \"''\", \"''ok''\", \"what's\", \"i'll\", \"it's\", \"i'm\", \"it's\", \"you're\", \"i'll\", \"i've\", \"can't\", \"didn't\", \"i'm\", \"i'm\", \"she'll\", \"i'm\", \"i'm\", \"don't\", \"i'm\", \"who's\", \"it's\", \"i've\", \"i'm\", \"you'll\", \"you'll\", \"i'm\", \"it's\", \"don't\", \"it's\", \"don't\", \"i'm\", \"can't\", \"i'm\", \"it'll\", \"i'll\", \"don't\", \"i'm\", \"i'm\", \"tt's\", \"i'm\", \"''\", \"''\", \"''ok''\", \"we're\", \"yetty's\", \"it's\", \"that's\", \"we're\", \"friend's\", \"haven't\", \"u're\", \"i'll\", \"she'll\", \"i'm\", \"they'll\", \"don't\", \"don't\", \"i'm\", \"i'm\", \"i'll\", \"today's\", \"i've\", \"don't\", \"it's\", \"don't\", \"mum's\", \"i'm\", \"wouldn't\", \"how's\", \"quote''\", \"joy's\", \"joy's\", \"don't\", \"don't\", \"i'd\", \"you've\", \"8'o\", \"don't\", \"'ll\", \"priscilla's\", \"how's\", \"everybody's\", \"there's\", \"wherre's\", \"i'll\", \"i'm\", \"cann't\", \"i'll\", \"doesn't\", \"it's\", \"can't\", \"i'm\", \"i'm\", \"i'm\", \"that'd\", \"i'll\", \"i'm\", \"i'll\", \"i'm\", \"who's\", \"you'll\", \"don't\", \"i'm\", \"ny's\", \"shahjahan's\", \"mumtaz's\", \"mumtaz's\", \"she's\", \"i'll\", \"how's\", \"i've\", \"she'll\", \"i'll\", \"we'll\", \"i'm\", \"joy's\", \"joy's\", \"i'll\", \"i'll\", \"don't\", \"doesn't\", \"i'll\", \"it's\", \"it's\", \"don't\", \"wat's\", \"he's\", \"it's\", \"didn't\", \"we're\", \"'maangalyam\", \"'\", \"you're\", \"what's\", \"i'm\", \"i'll\", \"i'm\", \"won't\", \"can't\", \"i've\", \"i'll\", \"er'ything\", \"i'll\", \"it's\", \"i'm\", \"i'm\", \"i'm\", \"i'm\", \"isn't\", \"hw'd\", \"wat'll\", \"espe'll\", \"'terrorist'\", \"i'm\", \"i'm\", \"doesn't\", \"you're\", \"i'm\", \"i'm\", \"i'd\", \"we'll\", \"that's\", \"didn't\", \"we'd\", \"mrng''\", \"don't\", \"don't\", \"'need'\", \"'comfort'\", \"'luxury'\", \"u'll\", \"it's\", \"armand's\", \"i'm\", \"can't\", \"i'm\", \"i'm\", \"i'm\", \"mom's\", \"i'll\", \"let's\", \"i'm\", \"that'll\", \"it's\", \"that's\", \"anything's\", \"i'm\", \"there's\", \"i'm\", \"we're\", \"guy's\", \"there're\", \"gumby's\", \"we're\", \"i'm\", \"you've\", \"it's\", \"i'm\", \"she's\", \"i'm\", \"don't\", \"haven't\", \"wasn't\", \"i'm\", \"i'll\", \"i'm\", \"she's\", \"there's\", \"won't\", \"i'm\", \"fakeye's\", \"i'm\", \"i'm\", \"that's\", \"you're\", \"'t\", \"i'll\", \"we'll\", \"he's\", \"won't\", \"there's\", \"i'm\", \"i'm\", \"i'm\", \"you're\", \"i'll\", \"can't\", \"that's\", \"can't\", \"wat's\", \"didn't\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"that's\", \"what's\", \"didn't\", \"you're\", \"we're\", \"i've\", \"blake's\", \"i'm\", \"i'm\", \"how's\", \"he's\", \"i'm\", \"i've\", \"i'll\", \"you're\", \"isn't\", \"ain't\", \"i'm\", \"i'm\", \"i'll\", \"i'll\", \"doesn't\", \"don't\", \"don't\", \"they're\", \"i'm\", \"aren't\", \"i'm\", \"i'll\", \"i'm\", \"you're\", \"can't\", \"that's\", \"i'm\", \"haven't\", \"what's\", \"how's\", \"i'm\", \"i'm\", \"i'll\", \"haven't\", \"he's\", \"i'm\", \"can't\", \"what's\", \"joke's\", \"i'll\", \"i've\", \"how's\", \"he's\", \"that's\", \"i'll\", \"won't\", \"i'm\", \"i'll\", \"i'll\", \"i'm\", \"it's\", \"i'm\", \"i'm\", \"i'm\", \"'terrorist'\", \"mine's\", \"i'm\", \"don't\", \"ugo's\", \"i'm\", \"friend's\", \"you're\", \"she's\", \"she's\", \"haven't\", \"partner's\", \"i'd\", \"b'day\", \"did'nt\", \"you'll\", \"nobody's\", \"can't\", \"she's\", \"aunty's\", \"i'm\", \"i've\", \"don't\", \"don't\", \"that's\", \"don't\", \"u're\", \"i'm\", \"she'll\", \"i'm\", \"wasn't\", \"i've\", \"can't\", \"cann't\", \"he's\", \"i'll\", \"i'll\", \"i'm\", \"wouldn't\", \"didn't\", \"don't\", \"can't\", \"i've\", \"don't\", \"can't\", \"dsn't\", \"dsn't\", \"dsn't\", \"don't\", \"i'd\", \"that's\", \"you're\", \"don't\", \"haven't\", \"i'm\", \"we're\", \"i'm\", \"i'll\", \"i'll\", \"b'day\", \"did'nt\", \"i'll\", \"i'm\", \"he's\", \"wudn't\", \"didn't\", \"can't\", \"it's\", \"ü'll\", \"i'ma\", \"i'll\", \"don't\", \"i've\", \"i'm\", \"it's\", \"don't\", \"i'm\", \"it's\", \"don't\", \"i'm\", \"i've\", \"i'm\", \"it's\", \"that's\", \"shit's\", \"he's\", \"''\", \"''\", \"''ok''\", \"i'm\", \"i'll\", \"i'm\", \"can't\", \"don't\", \"she's\", \"we've\", \"havn't\", \"i'm\", \"you're\", \"wat's\", \"i'm\", \"aren't\", \"haven't\", \"i've\", \"don't\", \"party's\", \"i've\", \"i've\", \"won't\", \"i'm\", \"what's\", \"i'm\", \"i'm\", \"didn't\", \"i'll\", \"i'll\", \"it's\", \"i'll\", \"i'm\", \"don't\", \"won't\", \"i'm\", \"couldn't\", \"i'm\", \"i'm\", \"i'll\", \"'that\", \"is'love'\", \"i'm\", \"i'm\", \"won't\", \"god's\", \"don't\", \"i'll\", \"i'm\", \"there's\", \"i'm\", \"i'm\", \"that's\", \"he'll\", \"you're\", \"we'll\", \"didn't\", \"how's\", \"i'll\", \"i'll\", \"i'm\", \"it's\", \"it's\", \"i'm\", \"there'll\", \"it's\", \"i've\", \"xin's\", \"it's\", \"i'm\", \"you're\", \"i'm\", \"don't\", \"he's\", \"i'm\", \"it's\", \"i'm\", \"wat's\", \"we're\", \"we'll\", \"we're\", \"i'm\", \"can't\", \"i'm\", \"don't\", \"we're\", \"i'm\", \"what's\", \"don't\", \"i'll\", \"i'm\", \"i'm\", \"how's\", \"it's\", \"i'll\", \"can't\", \"that's\", \"i'm\", \"you'd\", \"'heart'\", \"i'm\", \"wasn't\", \"didn't\", \"dip's\", \"world's\", \"tyler's\", \"i'm\", \"can't\", \"don't\", \"didn't\", \"fuck's\", \"she's\", \"didn't\", \"i've\", \"wasn't\", \"wasn't\", \"didn't\", \"wouldn't\", \"i'm\", \"you've\", \"it'snot\", \"child's\", \"they're\", \"i'm\", \"i've\", \"didn't\", \"don't\", \"don't\", \"how's\", \"where's\", \"how's\", \"i'm\", \"don't\", \"don't\", \"i'm\", \"i'm\", \"how's\", \"i'm\", \"didn't\", \"i'm\", \"i'm\", \"i'm\", \"i'm\", \"that's\", \"i'm\", \"i'm\", \"wasn't\", \"i'm\", \"there's\", \"olayiwola's\", \"'rencontre'\", \"i'm\", \"i'm\", \"it's\", \"you're\", \"she's\", \"didn't\", \"i'm\", \"i'm\", \"there's\", \"i'll\", \"dad's\", \"cann't\", \"i'm\", \"didn't\", \"b'coz\", \"you've\", \"virgil's\", \"i'm\", \"you're\", \"'hw\", \"'\", \"i've\", \"i'll\", \"you'll\", \"it's\", \"i'm\", \"i'm\", \"i'll\", \"i'm\", \"i'm\", \"i'm\", \"haven't\", \"sir's\", \"we'll\", \"it's\", \"i'm\", \"i'm\", \"i'll\", \"we'll\", \"what's\", \"i'm\", \"hasn't\", \"can't\", \"i'm\", \"i'll\", \"i'll\", \"it's\", \"i'm\", \"can't\", \"you'd\", \"it's\", \"i'm\", \"i'm\", \"he's\", \"i'm\", \"i'm\", \"how's\", \"don't\", \"haven't\", \"i'm\", \"can't\", \"i'm\", \"she's\", \"you'll\", \"it's\", \"it's\", \"it's\", \"i'm\", \"don't\", \"u'll\", \"u'll\", \"he's\", \"when's\", \"treatin'\", \"can't\", \"haven't\", \"i'm\", \"that's\", \"i'm\", \"i'm\", \"parents'\", \"i'm\", \"i'll\", \"how's\", \"i'm\", \"it's\", \"i'm\", \"shouldn't\", \"you're\", \"don't\", \"you'd\", \"wouldn't\", \"'til\", \"'\", \"that's\", \"i'm\", \"it's\", \"he's\", \"'simple'\", \"i'll\", \"i'm\", \"ryan's\", \"you're\", \"i'm\", \"i'm\", \"we'd\", \"didn't\", \"i'll\", \"don't\", \"i'm\", \"isn't\", \"weather's\", \"i'm\", \"don't\", \"i'm\", \"that's\", \"he's\", \"it's\", \"i'm\", \"u're\", \"when's\", \"there's\", \"don't\", \"how's\", \"i'm\", \"i'm\", \"mom's\", \"can't\", \"today's\", \"i'll\", \"it's\", \"you're\", \"wasn't\", \"you're\", \"i'm\", \"biola's\", \"i've\", \"i'd\", \"i'm\", \"that's\", \"i'll\", \"don't\", \"where's\", \"mummy's\", \"who's\", \"i'll\", \"god's\", \"god's\", \"god's\", \"god's\", \"i've\", \"i'm\", \"mei's\", \"i've\", \"tantrum's\", \"i'll\", \"can't\", \"it's\", \"i'm\", \"i'm\", \"you're\", \"i'll\", \"priscilla's\", \"that's\", \"i've\", \"that's\", \"you'll\", \"i'll\", \"aren't\", \"don't\", \"'til\", \"i'd\"]\n"
     ]
    }
   ],
   "source": [
    "# 구두점 특히 '(어퍼스트로피) 가 스팸메일에 얼마나 들어가고 주로 어떻게 사용되는지 확인해보기 (구두점이 실제 스팸 메일을 구별하는데 필요한지 확인하기 위함)\n",
    "# 구두점을 살리는 토크나이저로 가져와보기  -> tensorflow 의 keras 사용 \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "apost = []\n",
    "all_word_counts = 0 \n",
    "\n",
    "for contents in normal_contents:\n",
    "    tokens = text_to_word_sequence(contents)\n",
    "    all_word_counts+=len(tokens)\n",
    "    for t in tokens : \n",
    "        if \"'\" in t: \n",
    "            apost.append(t)\n",
    "\n",
    "print(\"normal 메일에서 token 의 개수 :\", all_word_counts)\n",
    "print(\"normal 메일에서 apostrophe 개수 : \", len(apost))\n",
    "print(\"normal 메일의 전체 토큰 중 apostrophe의 비율 :\", (len(apost) / all_word_counts) * 100)\n",
    "print(apost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[의견]\n",
    "\n",
    "그냥 어퍼스트로피만 모아서 봤을 때는 개수도 적고 필요없어보이긴하는데 스팸메일인지 햄메일인지 구별할 때 어떻게 사용될지 모르겠다.. \n",
    "\n",
    "normal 메일의 토큰과 spam 메일의 토큰 비교해보고 특징이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_apostrophe(text : str): # TODO 앞에 있는 코드도 이걸로 정리하기 \n",
    "    apost = [] # 어퍼스트로피 있는 것만 \n",
    "    refine_apost = [] # 어퍼스트로피 없는 것만\n",
    "    tokens = text_to_word_sequence(text)\n",
    "    for t in tokens : \n",
    "        if \"'\" in t: \n",
    "            apost.append(t)\n",
    "        else: \n",
    "            refine_apost.append(t)\n",
    "    return apost, refine_apost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스팸 메일 500 개의 토큰 개수 :  12582\n",
      "햄 메일 500 개의 토큰 개수 :  7083\n",
      "스팸 메일 중 자주 등장한 토큰 top 10개 [('to', 480), ('a', 252), ('call', 219), ('you', 193), ('your', 162), ('free', 154), ('now', 144), ('2', 133), ('for', 132), ('the', 132)]\n",
      "햄 메일 중 자주 등장한 토큰 top 10개 [('i', 261), ('you', 250), ('to', 149), ('the', 128), ('a', 125), ('my', 102), ('in', 87), ('and', 86), ('me', 81), ('u', 79)]\n"
     ]
    }
   ],
   "source": [
    "# 스팸메일 , 햄메일의 토큰 중 상위 10개만 확인해보기 \n",
    "from collections import Counter\n",
    "\n",
    "all_s_tokens = [] \n",
    "all_n_tokens = [] \n",
    "\n",
    "\n",
    "num = 500\n",
    "for s,n in zip(spam_contents[:num], normal_contents[:num]):\n",
    "    _,s_tokens = classify_apostrophe(s)\n",
    "    _,n_tokens = classify_apostrophe(n)\n",
    "    \n",
    "    all_s_tokens.extend(s_tokens)\n",
    "    all_n_tokens.extend(n_tokens)\n",
    "\n",
    "print(f\"스팸 메일 {num} 개의 토큰 개수 : \",len(all_s_tokens))\n",
    "print(f\"햄 메일 {num} 개의 토큰 개수 : \",len(all_n_tokens))\n",
    "\n",
    "print(\"스팸 메일 중 자주 등장한 토큰 top 10개\", Counter(all_s_tokens).most_common(10))\n",
    "print(\"햄 메일 중 자주 등장한 토큰 top 10개\", Counter(all_n_tokens).most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[의견]\n",
    "\n",
    "- 스팸메일의 길이가 햄메일보다 토큰의 길이가 길다. \n",
    "- 어퍼스트로피 없어도 무방할 것 같고, stopword와 길이가 짧은 단어(길이 1,2인 경우들) 제거가 필요함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
